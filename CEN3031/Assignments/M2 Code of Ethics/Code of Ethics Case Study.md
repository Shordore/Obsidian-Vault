
Case Study 1: [here](https://www.scu.edu/ethics/focus-areas/technology-ethics/resources/open-source-ai-to-release-or-not-to-release-the-gpt-2-synthetic-text-generator/)

OpenAI the makers of ChatGPT had fears of making their GPT-2 model open source over concerns of misuse by the public.  Initially, they released a reduced version of GPT-2 withholding the training data, and training code. They cited the potential harm to the public as their reasoning for this decision. In May 2019 OpenAI decided to release the full version instead citing that they found no "Strong evidence of misuse so far". OpenAI placed a disclaimer on the GitHub page stating that the model does not distinguish fact from fiction and should not be used when facts are important to the prompt. There are a few ACM ethics that could be used to help in this situation. ACM 1.1 would be helpful to make sure it is followed in this case, as the AI poses both a great value for society, but equally threatens the world as a whole, as the generated texts can be very damaging. ACM 1.2 is always an important one to follow as avoiding harm is generally a good goal to achieve, and can be applied here as AI has the potential to cause a lot of harm in the world. ACM 2.5 would be a crucial one to follow in evaluating the dangers of the program, and its impact. 2.5 applies to the situation directly as GPT-2 is not a simple program to release and its potential dangers are vast, so care must be put into evaluating the scale of the impact it could have. I believe overall the engineers did a good job preventing calamity from occurring during the release of GPT-2 as they took the time to evaluate the misuse during the partial release, before releasing the full version.


Case Study 2: [here](https://www.scu.edu/ethics/focus-areas/more-focus-areas/engineering-ethics/engineering-ethics-cases/a-violation-of-privacy/)

Marcus created an app that allows users to track their medical information and appointments. The company he works for has requested the user information to better target ads towards them, and Marcus is unsure of whether this would be ethical to do so, or how much information to provide. There are a few ACM codes that would apply to the situation, which could help Marcus navigate his dilemma. The most important code of ethics in this case would be ACM 1.6. Respecting the privacy of your users when creating an application that collects private health data is beyond important, it is paramount. Giving out this data to anyone besides medical personnel via the proper pathways would be highly questionable, if not illegal. A similar code that also applies here is 1.7 for a similar reason to the code before. Offering user data to anyone besides the appropriate authorities can be illegal, and often has serious consequences. As with almost every case in the software world, ACM 1.2 applies to this case, as avoiding harm can come in many forms, both physically, and emotionally. Giving out private information of users could be seen as causing harm, if this information ever appeared in a data breach as a result, it could cause significant harm to the public. Marcus clearly should not provide this information, as it could cause not only harm to the public, but could also land himself, and potentially the company in legal trouble. If Marcus were to share the information it would most definitely be unethical, and a clear violation of users' privacy.